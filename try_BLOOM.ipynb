{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6caef3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BloomForCausalLM, BloomTokenizerFast\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ba7b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bigscience/bloom-1b7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b85b625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo y el tokenizador\n",
    "model = BloomForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = BloomTokenizerFast.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e56b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexto = \"\"\"\n",
    "La historia comienza con la celebraci√≥n del mundo m√°gico. Durante muchos a√±os, los magos hab√≠an vivido aterrorizados por el malvado mago Lord Voldemort. La noche del 31 de octubre, mat√≥ a Lily y James Potter. Sin embargo, cuando intento matar a su hijo de un a√±o, Harry, la maldici√≥n asesina avada kedavra se vuelve sobre s√≠ mismo. El cuerpo de Voldemort resulta destruido, pero √©l sobrevive: se convierte en algo m√°s sutil que un fantasma, seg√∫n sus propias palabras, que no est√° ni muerto ni vivo. Por su parte, a Harry solo le queda una cicatriz con forma de rayo en la frente que es el √∫nico remanente f√≠sico de la maldici√≥n de Voldemort. Aparte de la cicatriz, a Harry le queda una extra√±a conexi√≥n cerebral con Voldemort, la cual hace que Harry pueda sentir las emociones de Voldemort y hable parsel, el idioma de las serpientes. Harry es el √∫nico sobreviviente de la maldici√≥n asesina, y a ra√≠z de la misteriosa derrota de Voldemort, el mundo m√°gico empieza a llamarlo como ¬´el ni√±o que sobrevivi√≥¬ª.\n",
    "El 2 de noviembre, Rubeus Hagrid, un semi-gigante, deja a Harry con los √∫nicos parientes que le quedan, sus crueles t√≠os los Dursley. Estos son su t√≠o Vernon, su t√≠a Petunia y Dudley, su primo malcriado. Ellos le ocultan el hecho de que es un mago y le dicen que sus padres han muerto en un accidente de coche. A Harry le castigan severamente despu√©s de cualquier comportamiento extra√±o. Sin embargo, la v√≠spera de su und√©cimo cumplea√±os, Harry tiene su primer contacto con el mundo m√°gico cuando recibe cartas del Colegio Hogwarts de Magia y Hechicer√≠a, las cuales eran entregadas por lechuzas, aunque su t√≠o impide que pueda leerlas. Ya en su cumplea√±os, Hagrid aparece y le dice a Harry que existe un mundo m√°gico, y, puesto que √©l es un mago, ha sido invitado a asistir al colegio Hogwarts de Magia y Hechicer√≠a.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54543985",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"who is voldemort? : \"+ contexto\n",
    "result_length = 100\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f73cf22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 425, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who is voldemort? : \n",
      "La historia comienza con la celebraci√≥n del mundo m√°gico. Durante muchos a√±os, los magos hab√≠an vivido aterrorizados por el malvado mago Lord Voldemort. La noche del 31 de octubre, mat√≥ a Lily y James Potter. Sin embargo, cuando intento matar a su hijo de un a√±o, Harry, la maldici√≥n asesina avada kedavra se vuelve sobre s√≠ mismo. El cuerpo de Voldemort resulta destruido, pero √©l sobrevive: se convierte en algo m√°s sutil que un fantasma, seg√∫n sus propias palabras, que no est√° ni muerto ni vivo. Por su parte, a Harry solo le queda una cicatriz con forma de rayo en la frente que es el √∫nico remanente f√≠sico de la maldici√≥n de Voldemort. Aparte de la cicatriz, a Harry le queda una extra√±a conexi√≥n cerebral con Voldemort, la cual hace que Harry pueda sentir las emociones de Voldemort y hable parsel, el idioma de las serpientes. Harry es el √∫nico sobreviviente de la maldici√≥n asesina, y a ra√≠z de la misteriosa derrota de Voldemort, el mundo m√°gico empieza a llamarlo como ¬´el ni√±o que sobrevivi√≥¬ª.\n",
      "El 2 de noviembre, Rubeus Hagrid, un semi-gigante, deja a Harry con los √∫nicos parientes que le quedan, sus crueles t√≠os los Dursley. Estos son su t√≠o Vernon, su t√≠a Petunia y Dudley, su primo malcriado. Ellos le ocultan el hecho de que es un mago y le dicen que sus padres han muerto en un accidente de coche. A Harry le castigan severamente despu√©s de cualquier comportamiento extra√±o. Sin embargo, la v√≠spera de su und√©cimo cumplea√±os, Harry tiene su primer contacto con el mundo m√°gico cuando recibe cartas del Colegio Hogwarts de Magia y Hechicer√≠a, las cuales eran entregadas por lechuzas, aunque su t√≠o impide que pueda leerlas. Ya en su cumplea√±os, Hagrid aparece y le dice a Harry que existe un mundo m√°gico, y, puesto que √©l es un mago, ha sido invitado a asistir al colegio Hogwarts de Magia y Hechicer√≠a.\n",
      "El\n"
     ]
    }
   ],
   "source": [
    "# Greedy Search\n",
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"], \n",
    "                       max_length=result_length\n",
    "                      )[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d24f9700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLa historia comienza con la celebraci√≥n del mundo m√°gico. Durante muchos a√±os, los magos hab√≠an vivido aterrorizados por el malvado mago Lord Voldemort. La noche del 31 de octubre, mat√≥ a Lily y James Potter. Sin embargo, cuando intento matar a su hijo de un a√±o, Harry, la maldici√≥n asesina avada kedavra se vuelve sobre s√≠ mismo. El cuerpo de Voldemort resulta destruido, pero √©l sobrevive: se convierte en algo m√°s sutil que un fantasma, seg√∫n sus propias palabras, que no est√° ni muerto ni vivo. Por su parte, a Harry solo le queda una cicatriz con forma de rayo en la frente que es el √∫nico remanente f√≠sico de la maldici√≥n de Voldemort. Aparte de la cicatriz, a Harry le queda una extra√±a conexi√≥n cerebral con Voldemort, la cual hace que Harry pueda sentir las emociones de Voldemort y hable parsel, el idioma de las serpientes. Harry es el √∫nico sobreviviente de la maldici√≥n asesina, y a ra√≠z de la misteriosa derrota de Voldemort, el mundo m√°gico empieza a llamarlo como ¬´el ni√±o que sobrevivi√≥¬ª.\\nEl 2 de noviembre, Rubeus Hagrid, un semi-gigante, deja a Harry con los √∫nicos parientes que le quedan, sus crueles t√≠os los Dursley. Estos son su t√≠o Vernon, su t√≠a Petunia y Dudley, su primo malcriado. Ellos le ocultan el hecho de que es un mago y le dicen que sus padres han muerto en un accidente de coche. A Harry le castigan severamente despu√©s de cualquier comportamiento extra√±o. Sin embargo, la v√≠spera de su und√©cimo cumplea√±os, Harry tiene su primer contacto con el mundo m√°gico cuando recibe cartas del Colegio Hogwarts de Magia y Hechicer√≠a, las cuales eran entregadas por lechuzas, aunque su t√≠o impide que pueda leerlas. Ya en su cumplea√±os, Hagrid aparece y le dice a Harry que existe un mundo m√°gico, y, puesto que √©l es un mago, ha sido invitado a asistir al colegio Hogwarts de Magia y Hechicer√≠a.\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d1ebcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬øCu√°l es el capital de Francia? ¬øEn qu√© se diferencia de los dem√°s pa√≠ses europeos? ¬øEn\n"
     ]
    }
   ],
   "source": [
    "# Definir la pregunta y codificarla\n",
    "question = \"¬øCu√°l es el capital de Francia?\"\n",
    "input_ids = tokenizer.encode(question, return_tensors='pt').to('cpu')\n",
    "\n",
    "# Obtener la respuesta del modelo\n",
    "output = model.generate(input_ids)\n",
    "answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(answer)  # Deber√≠a imprimir \"Paris\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040ef3b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015eb3a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02717a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d6a31dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"bigscience/bloomz-1b7\"\n",
    "#model_name = \"bigscience/bloomz-3b\"\n",
    "model_name = 'bigscience/bloomz-7b1'\n",
    "#model_name = \"bigscience/mt0-xxl-mt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "526d907f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 710/710 [00:00<00:00, 716kB/s]\n",
      "C:\\Users\\josep\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\josep\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.01G/6.01G [01:13<00:00, 81.2MB/s] \n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 199/199 [00:00<00:00, 100kB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14.5M/14.5M [00:01<00:00, 11.8MB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85.0/85.0 [00:00<00:00, 85.8kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "from transformers import BloomForCausalLM, BloomTokenizerFast\n",
    "\n",
    "# Cargar el modelo y el tokenizador\n",
    "model = BloomForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = BloomTokenizerFast.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1103776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definir el p√°rrafo de contexto y la pregunta, y codificarlos\n",
    "#context = \"Francia es un pa√≠s situado en Europa occidental, cuya capital es Paris.\"\n",
    "#question = \"¬øCu√°l es el idioma oficial de Francia?\"\n",
    "\n",
    "context = \"\"\"\n",
    "La historia comienza con la celebraci√≥n del mundo m√°gico. Durante muchos a√±os, los magos hab√≠an vivido aterrorizados por el malvado mago Lord Voldemort. La noche del 31 de octubre, mat√≥ a Lily y James Potter. Sin embargo, cuando intento matar a su hijo de un a√±o, Harry, la maldici√≥n asesina avada kedavra se vuelve sobre s√≠ mismo. El cuerpo de Voldemort resulta destruido, pero √©l sobrevive: se convierte en algo m√°s sutil que un fantasma, seg√∫n sus propias palabras, que no est√° ni muerto ni vivo. Por su parte, a Harry solo le queda una cicatriz con forma de rayo en la frente que es el √∫nico remanente f√≠sico de la maldici√≥n de Voldemort. Aparte de la cicatriz, a Harry le queda una extra√±a conexi√≥n cerebral con Voldemort, la cual hace que Harry pueda sentir las emociones de Voldemort y hable parsel, el idioma de las serpientes. Harry es el √∫nico sobreviviente de la maldici√≥n asesina, y a ra√≠z de la misteriosa derrota de Voldemort, el mundo m√°gico empieza a llamarlo como ¬´el ni√±o que sobrevivi√≥¬ª.\n",
    "El 2 de noviembre, Rubeus Hagrid, un semi-gigante, deja a Harry con los √∫nicos parientes que le quedan, sus crueles t√≠os los Dursley. Estos son su t√≠o Vernon, su t√≠a Petunia y Dudley, su primo malcriado. Ellos le ocultan el hecho de que es un mago y le dicen que sus padres han muerto en un accidente de coche. A Harry le castigan severamente despu√©s de cualquier comportamiento extra√±o. Sin embargo, la v√≠spera de su und√©cimo cumplea√±os, Harry tiene su primer contacto con el mundo m√°gico cuando recibe cartas del Colegio Hogwarts de Magia y Hechicer√≠a, las cuales eran entregadas por lechuzas, aunque su t√≠o impide que pueda leerlas. Ya en su cumplea√±os, Hagrid aparece y le dice a Harry que existe un mundo m√°gico, y, puesto que √©l es un mago, ha sido invitado a asistir al colegio Hogwarts de Magia y Hechicer√≠a.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5776b25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 426, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "La historia comienza con la celebraci√≥n del mundo m√°gico. Durante muchos a√±os, los magos hab√≠an vivido aterrorizados por el malvado mago Lord Voldemort. La noche del 31 de octubre, mat√≥ a Lily y James Potter. Sin embargo, cuando intento matar a su hijo de un a√±o, Harry, la maldici√≥n asesina avada kedavra se vuelve sobre s√≠ mismo. El cuerpo de Voldemort resulta destruido, pero √©l sobrevive: se convierte en algo m√°s sutil que un fantasma, seg√∫n sus propias palabras, que no est√° ni muerto ni vivo. Por su parte, a Harry solo le queda una cicatriz con forma de rayo en la frente que es el √∫nico remanente f√≠sico de la maldici√≥n de Voldemort. Aparte de la cicatriz, a Harry le queda una extra√±a conexi√≥n cerebral con Voldemort, la cual hace que Harry pueda sentir las emociones de Voldemort y hable parsel, el idioma de las serpientes. Harry es el √∫nico sobreviviente de la maldici√≥n asesina, y a ra√≠z de la misteriosa derrota de Voldemort, el mundo m√°gico empieza a llamarlo como ¬´el ni√±o que sobrevivi√≥¬ª.\n",
      "El 2 de noviembre, Rubeus Hagrid, un semi-gigante, deja a Harry con los √∫nicos parientes que le quedan, sus crueles t√≠os los Dursley. Estos son su t√≠o Vernon, su t√≠a Petunia y Dudley, su primo malcriado. Ellos le ocultan el hecho de que es un mago y le dicen que sus padres han muerto en un accidente de coche. A Harry le castigan severamente despu√©s de cualquier comportamiento extra√±o. Sin embargo, la v√≠spera de su und√©cimo cumplea√±os, Harry tiene su primer contacto con el mundo m√°gico cuando recibe cartas del Colegio Hogwarts de Magia y Hechicer√≠a, las cuales eran entregadas por lechuzas, aunque su t√≠o impide que pueda leerlas. Ya en su cumplea√±os, Hagrid aparece y le dice a Harry que existe un mundo m√°gico, y, puesto que √©l es un mago, ha sido invitado a asistir al colegio Hogwarts de Magia y Hechicer√≠a.\n",
      " quienes son los padres de harry? sus\n"
     ]
    }
   ],
   "source": [
    "#question = 'quien es el villano de la historia?'\n",
    "question = 'quienes son los padres de harry?'\n",
    "\n",
    "input_text = context + \" \" + question\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cpu')\n",
    "\n",
    "# Obtener la respuesta del modelo\n",
    "output = model.generate(input_ids)\n",
    "answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(answer)  # Deber√≠a imprimir \"franc√©s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657a711f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2345ebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigscience/bloomz-1b7\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25676350",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 426, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "La historia comienza con la celebraci√≥n del mundo m√°gico. Durante muchos a√±os, los magos hab√≠an vivido aterrorizados por el malvado mago Lord Voldemort. La noche del 31 de octubre, mat√≥ a Lily y James Potter. Sin embargo, cuando intento matar a su hijo de un a√±o, Harry, la maldici√≥n asesina avada kedavra se vuelve sobre s√≠ mismo. El cuerpo de Voldemort resulta destruido, pero √©l sobrevive: se convierte en algo m√°s sutil que un fantasma, seg√∫n sus propias palabras, que no est√° ni muerto ni vivo. Por su parte, a Harry solo le queda una cicatriz con forma de rayo en la frente que es el √∫nico remanente f√≠sico de la maldici√≥n de Voldemort. Aparte de la cicatriz, a Harry le queda una extra√±a conexi√≥n cerebral con Voldemort, la cual hace que Harry pueda sentir las emociones de Voldemort y hable parsel, el idioma de las serpientes. Harry es el √∫nico sobreviviente de la maldici√≥n asesina, y a ra√≠z de la misteriosa derrota de Voldemort, el mundo m√°gico empieza a llamarlo como ¬´el ni√±o que sobrevivi√≥¬ª.\n",
      "El 2 de noviembre, Rubeus Hagrid, un semi-gigante, deja a Harry con los √∫nicos parientes que le quedan, sus crueles t√≠os los Dursley. Estos son su t√≠o Vernon, su t√≠a Petunia y Dudley, su primo malcriado. Ellos le ocultan el hecho de que es un mago y le dicen que sus padres han muerto en un accidente de coche. A Harry le castigan severamente despu√©s de cualquier comportamiento extra√±o. Sin embargo, la v√≠spera de su und√©cimo cumplea√±os, Harry tiene su primer contacto con el mundo m√°gico cuando recibe cartas del Colegio Hogwarts de Magia y Hechicer√≠a, las cuales eran entregadas por lechuzas, aunque su t√≠o impide que pueda leerlas. Ya en su cumplea√±os, Hagrid aparece y le dice a Harry que existe un mundo m√°gico, y, puesto que √©l es un mago, ha sido invitado a asistir al colegio Hogwarts de Magia y Hechicer√≠a.\n",
      " quienes son los padres de harry? sus\n"
     ]
    }
   ],
   "source": [
    "question = 'quienes son los padres de harry?'\n",
    "input_text = context + \" \" + question\n",
    "\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dee1c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef38b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fb2421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a59273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4c1265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 430/430 [00:00<00:00, 433kB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.31M/4.31M [00:00<00:00, 4.71MB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16.3M/16.3M [00:01<00:00, 13.2MB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 74.0/74.0 [00:00<00:00, 149kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigscience/mt0-xxl-mt\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8767e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(\"Translate to English: Je t‚Äôaime.\", return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582a6f48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e78904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b1bcbb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>El idioma oficial de francia es</td>\n",
       "      <td>¬øCu√°l es el idioma oficial de francia?</td>\n",
       "      <td>franc√©s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>La capital de Italia es</td>\n",
       "      <td>¬øCu√°l es la capital de Italia?</td>\n",
       "      <td>Roma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El presidente de Estados Unidos es</td>\n",
       "      <td>¬øCu√°l es el presidente de Estados Unidos?</td>\n",
       "      <td>Joe Biden</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              context  \\\n",
       "0     El idioma oficial de francia es   \n",
       "1             La capital de Italia es   \n",
       "2  El presidente de Estados Unidos es   \n",
       "\n",
       "                                    question     answer  \n",
       "0     ¬øCu√°l es el idioma oficial de francia?    franc√©s  \n",
       "1             ¬øCu√°l es la capital de Italia?       Roma  \n",
       "2  ¬øCu√°l es el presidente de Estados Unidos?  Joe Biden  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Crear un diccionario de datos\n",
    "data = {\n",
    "    \"context\": [\"El idioma oficial de francia es\", \"La capital de Italia es\", \"El presidente de Estados Unidos es\"],\n",
    "    \"question\": [\"¬øCu√°l es el idioma oficial de francia?\", \"¬øCu√°l es la capital de Italia?\", \"¬øCu√°l es el presidente de Estados Unidos?\"],\n",
    "    \"answer\": [\"franc√©s\", \"Roma\", \"Joe Biden\"]\n",
    "}\n",
    "\n",
    "# Crear un DataFrame a partir del diccionario\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6b22f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdefcaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josep\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\josep\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate to English: Je t‚Äôaime. I love you.</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigscience/bloomz-1b7\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "\n",
    "inputs = tokenizer.encode(\"Translate to English: Je t‚Äôaime.\", return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd2f0f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josep\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\modeling_utils.py:2230\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m-> 2230\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(config, \u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_8bit:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1492\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m   1491\u001b[0m decoder_config\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mnum_decoder_layers\n\u001b[1;32m-> 1492\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mT5Stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshared\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39md_model, config\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:848\u001b[0m, in \u001b[0;36mT5Stack.__init__\u001b[1;34m(self, config, embed_tokens)\u001b[0m\n\u001b[0;32m    845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mis_decoder\n\u001b[0;32m    847\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m--> 848\u001b[0m     [T5Block(config, has_relative_attention_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_layers)]\n\u001b[0;32m    849\u001b[0m )\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm \u001b[38;5;241m=\u001b[39m T5LayerNorm(config\u001b[38;5;241m.\u001b[39md_model, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_epsilon)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:848\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mis_decoder\n\u001b[0;32m    847\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m--> 848\u001b[0m     [\u001b[43mT5Block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_relative_attention_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_layers)]\n\u001b[0;32m    849\u001b[0m )\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm \u001b[38;5;241m=\u001b[39m T5LayerNorm(config\u001b[38;5;241m.\u001b[39md_model, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_epsilon)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:634\u001b[0m, in \u001b[0;36mT5Block.__init__\u001b[1;34m(self, config, has_relative_attention_bias)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList()\n\u001b[1;32m--> 634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer\u001b[38;5;241m.\u001b[39mappend(\u001b[43mT5LayerSelfAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_relative_attention_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_relative_attention_bias\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:564\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.__init__\u001b[1;34m(self, config, has_relative_attention_bias)\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m--> 564\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSelfAttention \u001b[38;5;241m=\u001b[39m \u001b[43mT5Attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_relative_attention_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_relative_attention_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm \u001b[38;5;241m=\u001b[39m T5LayerNorm(config\u001b[38;5;241m.\u001b[39md_model, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_epsilon)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:349\u001b[0m, in \u001b[0;36mT5Attention.__init__\u001b[1;34m(self, config, has_relative_attention_bias)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner_dim, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 349\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\torch\\nn\\modules\\linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\torch\\nn\\modules\\linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\torch\\nn\\init.py:412\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[1;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39muniform_(\u001b[38;5;241m-\u001b[39mbound, bound)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigscience/mt0-xxl-mt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(checkpoint)\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslate to English: Je t‚Äôaime.\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(inputs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:463\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    462\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    464\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    465\u001b[0m     )\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    469\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\modeling_utils.py:2230\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2227\u001b[0m     init_contexts\u001b[38;5;241m.\u001b[39mappend(init_empty_weights())\n\u001b[0;32m   2229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m-> 2230\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(config, \u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_8bit:\n\u001b[0;32m   2233\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbitsandbytes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_keys_to_not_convert, replace_8bit_linear\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# pip install -q transformers\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigscience/mt0-xxl-mt\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "\n",
    "inputs = tokenizer.encode(\"Translate to English: Je t‚Äôaime.\", return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "74ca982e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a530e481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 199/199 [00:00<00:00, 201kB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14.5M/14.5M [00:01<00:00, 11.7MB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85.0/85.0 [00:00<00:00, 85.7kB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 742/742 [00:00<00:00, 748kB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14.1G/14.1G [03:57<00:00, 59.4MB/s]  \n",
      "Some weights of BloomForQuestionAnswering were not initialized from the model checkpoint at bigscience/bloomz-7b1 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "model_name = 'bigscience/bloomz-7b1'\n",
    "#model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained( model_name )\n",
    "model = AutoModelForQuestionAnswering.from_pretrained( model_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "211acbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many pretrained models are available in ü§ó Transformers?\n",
      "Answer: -bert) provides general-purpose\n",
      "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet‚Ä¶) for Natural Language Understanding (NLU) and Natural\n",
      "Language Generation (NLG) with over 32+ pre \n",
      "\n",
      "Question: What does ü§ó Transformers provide?\n",
      "Answer: -bert) provides general-purpose\n",
      "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet‚Ä¶) for Natural Language Understanding (NLU) and Natural\n",
      "Language Generation (NLG) with over 32+ pre \n",
      "\n",
      "Question: ü§ó Transformers provides interoperability between which frameworks?\n",
      "Answer: -bert) provides general-purpose\n",
      "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet‚Ä¶) for Natural Language Understanding (NLU) and Natural\n",
      "Language Generation (NLG) with over 32+ pre \n",
      "\n"
     ]
    }
   ],
   "source": [
    "context = r\"\"\"\n",
    "ü§ó Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet‚Ä¶) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"How many pretrained models are available in ü§ó Transformers?\",\n",
    "    \"What does ü§ó Transformers provide?\",\n",
    "    \"ü§ó Transformers provides interoperability between which frameworks?\",\n",
    "]\n",
    "\n",
    "def preguntar(question, context):\n",
    "    inputs = tokenizer(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "\n",
    "    # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    # Get the most likely end of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    "    )\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer} \\n\")\n",
    "\n",
    "    \n",
    "for question in questions:\n",
    "    preguntar(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ec762f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is NVMe\n",
      " Volume Serial Number is 80AD-482E\n",
      "\n",
      " Directory of C:\\Users\\josep\\Documents\\Python Scripts\\Untitled Folder\n",
      "\n",
      "01/09/2023  06:07 PM    <DIR>          .\n",
      "01/03/2023  12:28 PM    <DIR>          ..\n",
      "01/09/2023  11:07 AM    <DIR>          .ipynb_checkpoints\n",
      "01/03/2023  04:17 PM                 0 pygit_logger.log\n",
      "01/09/2023  06:07 PM            48,645 try_BLOOM.ipynb\n",
      "01/04/2023  04:49 PM           123,200 Untitled.ipynb\n",
      "               3 File(s)        171,845 bytes\n",
      "               3 Dir(s)  146,037,702,656 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5c47f44e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dev-v1.1-es.json'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    "\n",
    "link = \"https://raw.githubusercontent.com/ccasimiro88/TranslateAlignRetrieve/master/SQuAD-es-v1.1/dev-v1.1-es.json\"\n",
    "\n",
    "wget.download(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4234134",
   "metadata": {},
   "source": [
    "### Conjunto de datos\n",
    "\n",
    "* train-v1.1-es.json\n",
    "* dev-v1.1-es.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "313761df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('train-v1.1-es.json', 'r') as f:\n",
    "    datos = json.load(f)\n",
    "\n",
    "#datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83000e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datos[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1dcb92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datos[\"data\"][0][\"paragraphs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "540ec871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Arquitect√≥nicamente, la escuela tiene un car√°cter cat√≥lico. Encima de la c√∫pula de oro del edificio principal hay una estatua dorada de la Virgen Mar√≠a. Inmediatamente delante del edificio principal y frente a √©l, hay una estatua de cobre de Cristo con los brazos levantados con la leyenda \"Venite Ad Me Omnes\". Junto al edificio principal est√° la Bas√≠lica del Sagrado Coraz√≥n. Inmediatamente detr√°s de la bas√≠lica est√° la Gruta, un lugar mariano de oraci√≥n y reflexi√≥n. Es una r√©plica de la gruta de Lourdes, Francia, donde la Virgen Mar√≠a supuestamente se le apareci√≥ a Santa Bernadette Soubirous en 1858. Al final de la unidad principal (y en una l√≠nea directa que se conecta a trav√©s de 3 estatuas y la C√∫pula de Oro), hay una simple y moderna estatua de piedra de Mar√≠a.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos[\"data\"][0][\"paragraphs\"][0][\"context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b034d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': '¬øA qui√©n acudi√≥ la Virgen Mar√≠a supuestamente en 1858 en Lourdes France?',\n",
       "  'answers': [{'text': 'Santa Bernadette Soubirous', 'answer_start': 572}],\n",
       "  'id': '5733be284776f41900661182'},\n",
       " {'question': '¬øQu√© hay frente al edificio principal de Notre Dame?',\n",
       "  'answers': [{'text': 'una estatua de cobre de Cristo', 'answer_start': 218}],\n",
       "  'id': '5733be284776f4190066117f'},\n",
       " {'question': 'La Bas√≠lica del Sagrado Coraz√≥n en Notre Dame est√° al lado de qu√© estructura?',\n",
       "  'answers': [{'text': 'el edificio principal', 'answer_start': 88}],\n",
       "  'id': '5733be284776f41900661180'},\n",
       " {'question': '¬øQu√© es la Gruta de Notre Dame?',\n",
       "  'answers': [{'text': 'un lugar mariano de oraci√≥n y reflexi√≥n',\n",
       "    'answer_start': 430}],\n",
       "  'id': '5733be284776f41900661181'},\n",
       " {'question': '¬øQu√© se encuentra en la parte superior del edificio principal de Notre Dame?',\n",
       "  'answers': [{'text': 'una estatua dorada de la Virgen Mar√≠a',\n",
       "    'answer_start': 114}],\n",
       "  'id': '5733be284776f4190066117e'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos[\"data\"][0][\"paragraphs\"][0][\"qas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74184ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '¬øA qui√©n acudi√≥ la Virgen Mar√≠a supuestamente en 1858 en Lourdes France?',\n",
       " 'answers': [{'text': 'Santa Bernadette Soubirous', 'answer_start': 572}],\n",
       " 'id': '5733be284776f41900661182'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos[\"data\"][0][\"paragraphs\"][0][\"qas\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45e2d552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¬øA qui√©n acudi√≥ la Virgen Mar√≠a supuestamente en 1858 en Lourdes France?'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos[\"data\"][0][\"paragraphs\"][0][\"qas\"][0][\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca2bf611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Santa Bernadette Soubirous'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos[\"data\"][0][\"paragraphs\"][0][\"qas\"][0][\"answers\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14e38c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datos[\"data\"][0][\"paragraphs\"][0][\"qas\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6ea87f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_list = []\n",
    "question_list = []\n",
    "answers_list = []\n",
    "answer_list = []\n",
    "answer_start_list = []\n",
    "\n",
    "n = 0\n",
    "\n",
    "for dic in datos[\"data\"][0][\"paragraphs\"]:\n",
    "    n = len(dic[\"qas\"])\n",
    "    \n",
    "    for i in range(n):\n",
    "        context_list.append( dic[\"context\"] )\n",
    "        question_list.append( dic[\"qas\"][i][\"question\"] )\n",
    "        \n",
    "        answers_list.append( dic[\"qas\"][i][\"answers\"][0] )\n",
    "        \n",
    "        answer_list.append( dic[\"qas\"][i][\"answers\"][0][\"text\"] )\n",
    "        answer_start_list.append( dic[\"qas\"][i][\"answers\"][0][\"answer_start\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2eaa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdc223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = 'bigscience/bloomz-1b7'\n",
    "\n",
    "# Cargar el modelo y el tokenizador\n",
    "model = AutoModelForCausalLM.from_pretrained( model_name )\n",
    "tokenizer = AutoTokenizer.from_pretrained( model_name )\n",
    "\n",
    "def get_embeddings(text):\n",
    "    #se tokeniza y se convierte a ids\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    #se obtiene las salidas\n",
    "    last_hidden_states = model(input_ids)[0]\n",
    "    \n",
    "    return last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "27d2d86a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 7536640 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [46], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dic[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[1;32m---> 12\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdic\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m get_embeddings(dic[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqas\u001b[39m\u001b[38;5;124m\"\u001b[39m][i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     14\u001b[0m     x3 \u001b[38;5;241m=\u001b[39m get_embeddings(dic[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqas\u001b[39m\u001b[38;5;124m\"\u001b[39m][i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswers\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[1;32mIn [39], line 12\u001b[0m, in \u001b[0;36mget_embeddings\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     10\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#se obtiene las salidas\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m last_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m last_hidden_states\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\models\\bloom\\modeling_bloom.py:860\u001b[0m, in \u001b[0;36mBloomForCausalLM.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **deprecated_arguments)\u001b[0m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unexpected arguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeprecated_arguments\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    858\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 860\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    873\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\models\\bloom\\modeling_bloom.py:745\u001b[0m, in \u001b[0;36mBloomModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, **deprecated_arguments)\u001b[0m\n\u001b[0;32m    737\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    738\u001b[0m         create_custom_forward(block),\n\u001b[0;32m    739\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    742\u001b[0m         head_mask[i],\n\u001b[0;32m    743\u001b[0m     )\n\u001b[0;32m    744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 745\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m        \u001b[49m\u001b[43malibi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malibi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    755\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\models\\bloom\\modeling_bloom.py:464\u001b[0m, in \u001b[0;36mBloomBlock.forward\u001b[1;34m(self, hidden_states, alibi, attention_mask, layer_past, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    461\u001b[0m     residual \u001b[38;5;241m=\u001b[39m attention_output\n\u001b[0;32m    463\u001b[0m \u001b[38;5;66;03m# MLP.\u001b[39;00m\n\u001b[1;32m--> 464\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayernorm_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n\u001b[0;32m    467\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (output,) \u001b[38;5;241m+\u001b[39m outputs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\models\\bloom\\modeling_bloom.py:385\u001b[0m, in \u001b[0;36mBloomMLP.forward\u001b[1;34m(self, hidden_states, residual)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, residual: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 385\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgelu_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense_h_to_4h\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslow_but_exact:\n\u001b[0;32m    388\u001b[0m         intermediate_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(residual)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\models\\bloom\\modeling_bloom.py:209\u001b[0m, in \u001b[0;36mBloomGelu.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GeLUFunction\u001b[38;5;241m.\u001b[39mapply(x)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbloom_gelu_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\models\\bloom\\modeling_bloom.py:159\u001b[0m, in \u001b[0;36mbloom_gelu_forward\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbloom_gelu_forward\u001b[39m(x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03m    Custom bias GELU function. Adapted from Megatron-DeepSpeed code. Here we use a simple implementation (inference) to\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m    make the model jitable.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m            input hidden states\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.79788456\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.044715\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 7536640 bytes."
     ]
    }
   ],
   "source": [
    "context_list_cod = []\n",
    "question_list_cod = []\n",
    "answer_list_cod = []\n",
    "\n",
    "n = 0\n",
    "\n",
    "\n",
    "for dic in datos[\"data\"][0][\"paragraphs\"]:\n",
    "    n = len(dic[\"qas\"])\n",
    "    \n",
    "    for i in range(n):\n",
    "        x1 = get_embeddings(dic[\"context\"])\n",
    "        x2 = get_embeddings(dic[\"qas\"][i][\"question\"])\n",
    "        x3 = get_embeddings(dic[\"qas\"][i][\"answers\"][0][\"text\"])\n",
    "        \n",
    "        context_list_cod .append( x1 )\n",
    "        question_list_cod.append( x2 )\n",
    "        answer_list_cod  .append( x3 )\n",
    "        \n",
    "        del x1, x2, x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c14aac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arquitect√≥nicamente, la escuela tiene un car√°c...</td>\n",
       "      <td>¬øA qui√©n acudi√≥ la Virgen Mar√≠a supuestamente ...</td>\n",
       "      <td>{'text': 'Santa Bernadette Soubirous', 'answer...</td>\n",
       "      <td>Santa Bernadette Soubirous</td>\n",
       "      <td>572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arquitect√≥nicamente, la escuela tiene un car√°c...</td>\n",
       "      <td>¬øQu√© hay frente al edificio principal de Notre...</td>\n",
       "      <td>{'text': 'una estatua de cobre de Cristo', 'an...</td>\n",
       "      <td>una estatua de cobre de Cristo</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arquitect√≥nicamente, la escuela tiene un car√°c...</td>\n",
       "      <td>La Bas√≠lica del Sagrado Coraz√≥n en Notre Dame ...</td>\n",
       "      <td>{'text': 'el edificio principal', 'answer_star...</td>\n",
       "      <td>el edificio principal</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arquitect√≥nicamente, la escuela tiene un car√°c...</td>\n",
       "      <td>¬øQu√© es la Gruta de Notre Dame?</td>\n",
       "      <td>{'text': 'un lugar mariano de oraci√≥n y reflex...</td>\n",
       "      <td>un lugar mariano de oraci√≥n y reflexi√≥n</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arquitect√≥nicamente, la escuela tiene un car√°c...</td>\n",
       "      <td>¬øQu√© se encuentra en la parte superior del edi...</td>\n",
       "      <td>{'text': 'una estatua dorada de la Virgen Mar√≠...</td>\n",
       "      <td>una estatua dorada de la Virgen Mar√≠a</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>Los alumnos de Notre Dame trabajan en varios c...</td>\n",
       "      <td>¬øQu√© Secretario de Estado asisti√≥ a Notre Dame?</td>\n",
       "      <td>{'text': 'Condoleezza Rice', 'answer_start': 229}</td>\n",
       "      <td>Condoleezza Rice</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>Los alumnos de Notre Dame trabajan en varios c...</td>\n",
       "      <td>¬øCu√°l alumbre de Notre Dame del Colegio de Cie...</td>\n",
       "      <td>{'text': 'Eric F. Wieschaus', 'answer_start': ...</td>\n",
       "      <td>Eric F. Wieschaus</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>Los alumnos de Notre Dame trabajan en varios c...</td>\n",
       "      <td>¬øQui√©n es el actual presidente de Notre Dame?</td>\n",
       "      <td>{'text': 'John Jenkins', 'answer_start': 451}</td>\n",
       "      <td>John Jenkins</td>\n",
       "      <td>451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>Los alumnos de Notre Dame trabajan en varios c...</td>\n",
       "      <td>Mariel Zagunis es notable por ganar ¬øqu√©?</td>\n",
       "      <td>{'text': 'Joe', 'answer_start': 898}</td>\n",
       "      <td>Joe</td>\n",
       "      <td>898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>Los alumnos de Notre Dame trabajan en varios c...</td>\n",
       "      <td>¬øQu√© astronauta notable ha asistido a Notre Dame?</td>\n",
       "      <td>{'text': 'astronauta Ji', 'answer_start': 998}</td>\n",
       "      <td>astronauta Ji</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>269 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               context  \\\n",
       "0    Arquitect√≥nicamente, la escuela tiene un car√°c...   \n",
       "1    Arquitect√≥nicamente, la escuela tiene un car√°c...   \n",
       "2    Arquitect√≥nicamente, la escuela tiene un car√°c...   \n",
       "3    Arquitect√≥nicamente, la escuela tiene un car√°c...   \n",
       "4    Arquitect√≥nicamente, la escuela tiene un car√°c...   \n",
       "..                                                 ...   \n",
       "264  Los alumnos de Notre Dame trabajan en varios c...   \n",
       "265  Los alumnos de Notre Dame trabajan en varios c...   \n",
       "266  Los alumnos de Notre Dame trabajan en varios c...   \n",
       "267  Los alumnos de Notre Dame trabajan en varios c...   \n",
       "268  Los alumnos de Notre Dame trabajan en varios c...   \n",
       "\n",
       "                                              question  \\\n",
       "0    ¬øA qui√©n acudi√≥ la Virgen Mar√≠a supuestamente ...   \n",
       "1    ¬øQu√© hay frente al edificio principal de Notre...   \n",
       "2    La Bas√≠lica del Sagrado Coraz√≥n en Notre Dame ...   \n",
       "3                      ¬øQu√© es la Gruta de Notre Dame?   \n",
       "4    ¬øQu√© se encuentra en la parte superior del edi...   \n",
       "..                                                 ...   \n",
       "264    ¬øQu√© Secretario de Estado asisti√≥ a Notre Dame?   \n",
       "265  ¬øCu√°l alumbre de Notre Dame del Colegio de Cie...   \n",
       "266      ¬øQui√©n es el actual presidente de Notre Dame?   \n",
       "267          Mariel Zagunis es notable por ganar ¬øqu√©?   \n",
       "268  ¬øQu√© astronauta notable ha asistido a Notre Dame?   \n",
       "\n",
       "                                               answers  \\\n",
       "0    {'text': 'Santa Bernadette Soubirous', 'answer...   \n",
       "1    {'text': 'una estatua de cobre de Cristo', 'an...   \n",
       "2    {'text': 'el edificio principal', 'answer_star...   \n",
       "3    {'text': 'un lugar mariano de oraci√≥n y reflex...   \n",
       "4    {'text': 'una estatua dorada de la Virgen Mar√≠...   \n",
       "..                                                 ...   \n",
       "264  {'text': 'Condoleezza Rice', 'answer_start': 229}   \n",
       "265  {'text': 'Eric F. Wieschaus', 'answer_start': ...   \n",
       "266      {'text': 'John Jenkins', 'answer_start': 451}   \n",
       "267               {'text': 'Joe', 'answer_start': 898}   \n",
       "268     {'text': 'astronauta Ji', 'answer_start': 998}   \n",
       "\n",
       "                                      answer  answer_start  \n",
       "0                 Santa Bernadette Soubirous           572  \n",
       "1             una estatua de cobre de Cristo           218  \n",
       "2                      el edificio principal            88  \n",
       "3    un lugar mariano de oraci√≥n y reflexi√≥n           430  \n",
       "4      una estatua dorada de la Virgen Mar√≠a           114  \n",
       "..                                       ...           ...  \n",
       "264                         Condoleezza Rice           229  \n",
       "265                        Eric F. Wieschaus           335  \n",
       "266                             John Jenkins           451  \n",
       "267                                      Joe           898  \n",
       "268                            astronauta Ji           998  \n",
       "\n",
       "[269 rows x 5 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame( {\"context\"      : context_list,\n",
    "                    \"question\"     : question_list,\n",
    "                    \"answers\"      : answers_list,\n",
    "                    \"answer\"      : answer_list,\n",
    "                    \"answer_start\" : answer_start_list} )\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe147489",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame( {\"context\"  : context_list_cod,\n",
    "                    \"question\" : question_list_cod,\n",
    "                    \"answers\"  : answer_list_cod} )\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "620ee43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"train_SQuAD_es_11.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "972982ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arquitect√≥nicamente, la escuela tiene un car√°c...</td>\n",
       "      <td>¬øA qui√©n acudi√≥ la Virgen Mar√≠a supuestamente ...</td>\n",
       "      <td>{'text': 'Santa Bernadette Soubirous', 'answer...</td>\n",
       "      <td>Santa Bernadette Soubirous</td>\n",
       "      <td>572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arquitect√≥nicamente, la escuela tiene un car√°c...</td>\n",
       "      <td>¬øQu√© hay frente al edificio principal de Notre...</td>\n",
       "      <td>{'text': 'una estatua de cobre de Cristo', 'an...</td>\n",
       "      <td>una estatua de cobre de Cristo</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arquitect√≥nicamente, la escuela tiene un car√°c...</td>\n",
       "      <td>La Bas√≠lica del Sagrado Coraz√≥n en Notre Dame ...</td>\n",
       "      <td>{'text': 'el edificio principal', 'answer_star...</td>\n",
       "      <td>el edificio principal</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arquitect√≥nicamente, la escuela tiene un car√°c...</td>\n",
       "      <td>¬øQu√© es la Gruta de Notre Dame?</td>\n",
       "      <td>{'text': 'un lugar mariano de oraci√≥n y reflex...</td>\n",
       "      <td>un lugar mariano de oraci√≥n y reflexi√≥n</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arquitect√≥nicamente, la escuela tiene un car√°c...</td>\n",
       "      <td>¬øQu√© se encuentra en la parte superior del edi...</td>\n",
       "      <td>{'text': 'una estatua dorada de la Virgen Mar√≠...</td>\n",
       "      <td>una estatua dorada de la Virgen Mar√≠a</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  Arquitect√≥nicamente, la escuela tiene un car√°c...   \n",
       "1  Arquitect√≥nicamente, la escuela tiene un car√°c...   \n",
       "2  Arquitect√≥nicamente, la escuela tiene un car√°c...   \n",
       "3  Arquitect√≥nicamente, la escuela tiene un car√°c...   \n",
       "4  Arquitect√≥nicamente, la escuela tiene un car√°c...   \n",
       "\n",
       "                                            question  \\\n",
       "0  ¬øA qui√©n acudi√≥ la Virgen Mar√≠a supuestamente ...   \n",
       "1  ¬øQu√© hay frente al edificio principal de Notre...   \n",
       "2  La Bas√≠lica del Sagrado Coraz√≥n en Notre Dame ...   \n",
       "3                    ¬øQu√© es la Gruta de Notre Dame?   \n",
       "4  ¬øQu√© se encuentra en la parte superior del edi...   \n",
       "\n",
       "                                             answers  \\\n",
       "0  {'text': 'Santa Bernadette Soubirous', 'answer...   \n",
       "1  {'text': 'una estatua de cobre de Cristo', 'an...   \n",
       "2  {'text': 'el edificio principal', 'answer_star...   \n",
       "3  {'text': 'un lugar mariano de oraci√≥n y reflex...   \n",
       "4  {'text': 'una estatua dorada de la Virgen Mar√≠...   \n",
       "\n",
       "                                    answer  answer_start  \n",
       "0               Santa Bernadette Soubirous           572  \n",
       "1           una estatua de cobre de Cristo           218  \n",
       "2                    el edificio principal            88  \n",
       "3  un lugar mariano de oraci√≥n y reflexi√≥n           430  \n",
       "4    una estatua dorada de la Virgen Mar√≠a           114  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv( \"train_SQuAD_es_11.csv\" )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6f49bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'Los alumnos de Notre Dame trabajan en varios campos. Los ex alumnos que trabajan en campos pol√≠ticos incluyen gobernadores estatales, miembros del Congreso de los Estados Unidos y la ex Secretaria de Estado de los Estados Unidos Condoleezza Rice. Un alumno notable de la Facultad de Ciencias es el ganador del Premio Nobel de Medicina Eric F. Wieschaus. Varios jefes universitarios son ex alumnos, incluido el actual presidente de Notre Dame, el Rev. John Jenkins. Adem√°s, muchos ex alumnos est√°n en los medios, incluyendo los presentadores de programas de entrevistas Regis Philbin y Phil Donahue, y personalidades de televisi√≥n y radio como Mike Golic y Hannah Storm. Con la universidad teniendo equipos deportivos de alto perfil, varios ex alumnos se involucraron en atletismo fuera de la universidad, incluyendo b√©isbol profesional, baloncesto, f√∫tbol y hockey sobre hielo, como Joe Theismann, Joe Montana. Otros alumnos notables incluyen al prominente empresario Edward J. DeBartolo, Jr. y al astronauta Jim Wetherbee.',\n",
       " 'qas': [{'question': '¬øQu√© Secretario de Estado asisti√≥ a Notre Dame?',\n",
       "   'answers': [{'text': 'Condoleezza Rice', 'answer_start': 229}],\n",
       "   'id': '5733cd504776f4190066128e'},\n",
       "  {'question': '¬øCu√°l alumbre de Notre Dame del Colegio de Ciencias gan√≥ un Premio Nobel?',\n",
       "   'answers': [{'text': 'Eric F. Wieschaus', 'answer_start': 335}],\n",
       "   'id': '5733cd504776f4190066128f'},\n",
       "  {'question': '¬øQui√©n es el actual presidente de Notre Dame?',\n",
       "   'answers': [{'text': 'John Jenkins', 'answer_start': 451}],\n",
       "   'id': '5733cd504776f41900661290'},\n",
       "  {'question': 'Mariel Zagunis es notable por ganar ¬øqu√©?',\n",
       "   'answers': [{'text': 'Joe', 'answer_start': 898}],\n",
       "   'id': '5733cd504776f41900661291'},\n",
       "  {'question': '¬øQu√© astronauta notable ha asistido a Notre Dame?',\n",
       "   'answers': [{'text': 'astronauta Ji', 'answer_start': 998}],\n",
       "   'id': '5733cd504776f41900661292'}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos[\"data\"][0][\"paragraphs\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c1d462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67981401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be51d364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c172437d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46447d29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa64406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbf707eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "208",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 208",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [16], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# Entrenar una √©poca\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m context, question, answer \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;66;03m# Codificar los datos de entrada\u001b[39;00m\n\u001b[0;32m     39\u001b[0m         input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(context \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m question, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     40\u001b[0m         target_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(answer, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\pandas\\core\\frame.py:3804\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3804\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3806\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 208"
     ]
    }
   ],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "import torch\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer, AdamW\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = 'bigscience/bloomz-1b7'\n",
    "\n",
    "# Cargar el modelo y el tokenizador\n",
    "model = AutoModelForCausalLM.from_pretrained( model_name )\n",
    "tokenizer = AutoTokenizer.from_pretrained( model_name )\n",
    "\n",
    "# Definir el conjunto de datos y los hiperpar√°metros\n",
    "train_data = df.iloc[:-20 , :] # tus datos de entrenamiento\n",
    "val_data = df.iloc[-20: , :]# tus datos de validaci√≥n\n",
    "batch_size = 16  # tama√±o del lote\n",
    "lr = 2e-5  # tasa de aprendizaje\n",
    "num_epochs = 5  # n√∫mero de √©pocas\n",
    "\n",
    "# Crear un dataloader para el conjunto de datos de entrenamiento\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "# Crear un dataloader para el conjunto de datos de validaci√≥n\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_data, batch_size=batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "# Configurar el optimizador\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# Entrenar el modelo\n",
    "for epoch in range(num_epochs):\n",
    "    # Entrenar una √©poca\n",
    "    model.train()\n",
    "    for context, question, answer in train_dataloader:\n",
    "        # Codificar los datos de entrada\n",
    "        input_ids = tokenizer.encode(context + \" \" + question, return_tensors='pt').to('')\n",
    "        target_ids = tokenizer.encode(answer, return_tensors='pt').to('')\n",
    "\n",
    "        # Obtener la salida del modelo\n",
    "        output = model(input_ids, labels=target_ids)\n",
    "        loss = output[0]\n",
    "\n",
    "        # Realizar el backpropagation y optimizaci√≥n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Validar el modelo\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for context, question, answer in val_dataloader:\n",
    "            # Codificar los datos de entrada\n",
    "            input_ids = tokenizer.encode(context + \" \" + question, return_tensors='pt').to('')\n",
    "            target_ids = tokenizer.encode(answer, return_tensors='pt').to('')\n",
    "\n",
    "            # Obtener la salida del modelo\n",
    "            output = model(input_ids, labels=target_ids)\n",
    "            loss = output[0]\n",
    "\n",
    "            # Calcula la p√©rdida y la precisi√≥n promedio\n",
    "            val_loss += loss.item()\n",
    "            val_accuracy += (output[1].argmax(1) == target_ids).mean().item()\n",
    "\n",
    "        # Imprime la p√©rdida y la precisi√≥n promedio\n",
    "        val_loss /= len(val_dataloader)\n",
    "        val_accuracy /= len(val_dataloader)\n",
    "        print(f\"Epoch {epoch+1}: val_loss = {val_loss:.4f}, val_accuracy = {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34fc6be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e5daf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cca10a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2123d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "#model_name = 'bigscience/bloomz-1b7'\n",
    "\n",
    "# a) Get predictions\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "QA_input = {\n",
    "    'question': 'Why is model conversion important?',\n",
    "    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n",
    "}\n",
    "res = nlp(QA_input)\n",
    "\n",
    "# b) Load model & tokenizer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02f05de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.21171435713768005,\n",
       " 'start': 59,\n",
       " 'end': 84,\n",
       " 'answer': 'gives freedom to the user'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res  # \"deepset/roberta-base-squad2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11653a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.010970551520586014,\n",
       " 'start': 36,\n",
       " 'end': 106,\n",
       " 'answer': ' FARM and transformers gives freedom to the user and let people easily'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res  # 'bigscience/bloomz-1b7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "535671fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fine_tuning_roberta_QA_blog.ipynb'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    "\n",
    "wget.download( \"https://github.com/skandavivek/transformerQA-finetuning/blob/main/fine_tuning_roberta_QA_blog.ipynb\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc1b37d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67b0de7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = 'bigscience/bloomz-1b7'\n",
    "\n",
    "# Cargar el modelo y el tokenizador\n",
    "model = AutoModelForCausalLM.from_pretrained( model_name )\n",
    "tokenizer = AutoTokenizer.from_pretrained( model_name )\n",
    "\n",
    "def get_embeddings(text):\n",
    "    #se tokeniza y se convierte a ids\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    #se obtiene las salidas\n",
    "    last_hidden_states = model(input_ids)[0]\n",
    "    \n",
    "    return last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "598d8588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[249603,   2745,    616,   1035,  49412,    283,  34689,    852,  11989,\n",
       "           1564,    458,  31346,   5665,    393, 113851,   6057, 218908]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b4d361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text):\n",
    "    #se tokeniza y se convierte a ids\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    #se obtiene las salidas\n",
    "    last_hidden_states = model(input_ids)[0]\n",
    "    \n",
    "    return last_hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a6e8b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['context', 'question', 'answers'], dtype='object')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 7536640 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [40], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m dataframe \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_SQuAD_es_11.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataframe\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m----> 4\u001b[0m dataframe_cod \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame( {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext_cod\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[43mdataframe\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m      5\u001b[0m                                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion_cod\u001b[39m\u001b[38;5;124m\"\u001b[39m: dataframe[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply( get_embeddings ),\n\u001b[0;32m      6\u001b[0m                                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswers_cod\u001b[39m\u001b[38;5;124m\"\u001b[39m : dataframe[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswers\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply( get_embeddings )} )\n\u001b[0;32m      7\u001b[0m dataframe_cod\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\pandas\\core\\apply.py:1105\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\pandas\\core\\apply.py:1156\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1154\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1155\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1156\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1163\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2918\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn [39], line 12\u001b[0m, in \u001b[0;36mget_embeddings\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     10\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#se obtiene las salidas\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m last_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m last_hidden_states\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\models\\bloom\\modeling_bloom.py:860\u001b[0m, in \u001b[0;36mBloomForCausalLM.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **deprecated_arguments)\u001b[0m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unexpected arguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeprecated_arguments\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    858\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 860\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    873\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\models\\bloom\\modeling_bloom.py:745\u001b[0m, in \u001b[0;36mBloomModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, **deprecated_arguments)\u001b[0m\n\u001b[0;32m    737\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    738\u001b[0m         create_custom_forward(block),\n\u001b[0;32m    739\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    742\u001b[0m         head_mask[i],\n\u001b[0;32m    743\u001b[0m     )\n\u001b[0;32m    744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 745\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m        \u001b[49m\u001b[43malibi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malibi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    755\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\models\\bloom\\modeling_bloom.py:464\u001b[0m, in \u001b[0;36mBloomBlock.forward\u001b[1;34m(self, hidden_states, alibi, attention_mask, layer_past, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    461\u001b[0m     residual \u001b[38;5;241m=\u001b[39m attention_output\n\u001b[0;32m    463\u001b[0m \u001b[38;5;66;03m# MLP.\u001b[39;00m\n\u001b[1;32m--> 464\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayernorm_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n\u001b[0;32m    467\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (output,) \u001b[38;5;241m+\u001b[39m outputs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\models\\bloom\\modeling_bloom.py:385\u001b[0m, in \u001b[0;36mBloomMLP.forward\u001b[1;34m(self, hidden_states, residual)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, residual: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 385\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgelu_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense_h_to_4h\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslow_but_exact:\n\u001b[0;32m    388\u001b[0m         intermediate_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(residual)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\models\\bloom\\modeling_bloom.py:209\u001b[0m, in \u001b[0;36mBloomGelu.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GeLUFunction\u001b[38;5;241m.\u001b[39mapply(x)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbloom_gelu_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\models\\bloom\\modeling_bloom.py:159\u001b[0m, in \u001b[0;36mbloom_gelu_forward\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbloom_gelu_forward\u001b[39m(x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03m    Custom bias GELU function. Adapted from Megatron-DeepSpeed code. Here we use a simple implementation (inference) to\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m    make the model jitable.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m            input hidden states\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;241m0.79788456\u001b[39m \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.044715\u001b[39m \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m*\u001b[39m x)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 7536640 bytes."
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv(\"train_SQuAD_es_11.csv\")\n",
    "print(dataframe.columns)\n",
    "\n",
    "dataframe_cod = pd.DataFrame( {\"context_cod\" : dataframe[\"context\"].apply( get_embeddings ),\n",
    "                               \"question_cod\": dataframe[\"question\"].apply( get_embeddings ),\n",
    "                               \"answers_cod\" : dataframe[\"answers\"].apply( get_embeddings )} )\n",
    "dataframe_cod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e023cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_cod.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f67b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, Trainer, TrainingArguments\n",
    "\n",
    "# configure the model\n",
    "model_name = 'bigscience/bloomz-1b7'\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-cased-distilled-squad\")\n",
    "\n",
    "# define the data to train on\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f82d6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b021160e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [26], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m answer \u001b[38;5;241m=\u001b[39m dataframe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswers\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# utilizamos torch.tensor() para convertir las listas a tensores\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m question \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(question)\n\u001b[0;32m     15\u001b[0m answer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(answer)\n",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "\n",
    "# Utilizamos pandas para leer el archivo csv y crear un dataframe\n",
    "dataframe = pd.read_csv(\"train_SQuAD_es_11.csv\")\n",
    "\n",
    "# convertimos las columnas context,question y answer a una lista \n",
    "context = dataframe['context'].tolist()\n",
    "question = dataframe['question'].tolist()\n",
    "answer = dataframe['answers'].tolist()\n",
    "\n",
    "# utilizamos torch.tensor() para convertir las listas a tensores\n",
    "context = torch.tensor(context)\n",
    "question = torch.tensor(question)\n",
    "answer = torch.tensor(answer)\n",
    "\n",
    "# Creamos el dataset a partir de los tensores\n",
    "train_dataset = TensorDataset(context, question, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616f454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4523b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "# start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37803c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51d00ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, TrainingArguments\n",
    "\n",
    "# Leer el archivo .csv con pandas\n",
    "data = pd.read_csv(\"train_SQuAD_es_11.csv\")\n",
    "\n",
    "# Convertir los datos a un formato compatible con el modelo\n",
    "training_data = [\n",
    "    {\"context\": row[\"context\"], \"question\": row[\"question\"], \"answer\": row[\"answers\"]}\n",
    "    for _, row in data.iterrows()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac283ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una instancia del modelo y del tokenizador\n",
    "model_name = 'bigscience/bloomz-1b7'\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a87f8f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar los argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=4e-5,\n",
    "    overwrite_output_dir=True,\n",
    "    #evaluate_during_training=True,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b085d9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "training mode is expected to be boolean",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1904\u001b[0m, in \u001b[0;36mModule.train\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m   1889\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sets the module in training mode.\u001b[39;00m\n\u001b[0;32m   1890\u001b[0m \n\u001b[0;32m   1891\u001b[0m \u001b[38;5;124;03mThis has any effect only on certain modules. See documentations of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1901\u001b[0m \u001b[38;5;124;03m    Module: self\u001b[39;00m\n\u001b[0;32m   1902\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mode, \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m-> 1904\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining mode is expected to be boolean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1905\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m   1906\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n",
      "\u001b[1;31mValueError\u001b[0m: training mode is expected to be boolean"
     ]
    }
   ],
   "source": [
    "model.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1f44242",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BloomForQuestionAnswering' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Entrenar el modelo\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m(training_data, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, args\u001b[38;5;241m=\u001b[39mtraining_args)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1265\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1264\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1265\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1266\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BloomForQuestionAnswering' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "model.fit(training_data, tokenizer=tokenizer, args=training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e7f247f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BloomForQuestionAnswering were not initialized from the model checkpoint at bigscience/bloomz-1b7 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BloomForQuestionAnswering were not initialized from the model checkpoint at bigscience/bloomz-1b7 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "#model_name = \"deepset/roberta-base-squad2\"\n",
    "model_name = 'bigscience/bloomz-1b7'\n",
    "\n",
    "# a) Get predictions\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "QA_input = {\n",
    "    'question': 'Why is model conversion important?',\n",
    "    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n",
    "}\n",
    "res = nlp(QA_input)\n",
    "\n",
    "# b) Load model & tokenizer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55a665b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.21171435713768005,\n",
       " 'start': 59,\n",
       " 'end': 84,\n",
       " 'answer': 'gives freedom to the user'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85f589b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0008815079345367849, 'start': 10, 'end': 13, 'answer': ' to'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08695c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f9f938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dfad40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b5f3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47c6618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93b28b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!export CUDA_VISIBLE_DEVICES=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6a17dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josep\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BloomForQuestionAnswering were not initialized from the model checkpoint at bigscience/bloomz-560m and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, BloomForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "#model_name = 'bigscience/bloomz-1b1'\n",
    "model_name = 'bigscience/bloomz-560m'\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name).to(\"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61d7cdb",
   "metadata": {},
   "source": [
    "# Leemos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22add62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def json_2_df(file_json):\n",
    "    with open(file_json, 'r') as f:\n",
    "        datos = json.load(f)\n",
    "\n",
    "    context_list = []\n",
    "    question_list = []\n",
    "    answers_list = []\n",
    "    answer_list = []\n",
    "    answer_start_list = []\n",
    "\n",
    "    n = 0\n",
    "\n",
    "    for dic in datos[\"data\"][0][\"paragraphs\"]:\n",
    "        n = len(dic[\"qas\"])\n",
    "\n",
    "        for i in range(n):\n",
    "            context_list.append( dic[\"context\"] )\n",
    "            question_list.append( dic[\"qas\"][i][\"question\"] )\n",
    "\n",
    "            answers_list.append( dic[\"qas\"][i][\"answers\"][0] )\n",
    "\n",
    "            answer_list.append( dic[\"qas\"][i][\"answers\"][0][\"text\"] )\n",
    "            answer_start_list.append( dic[\"qas\"][i][\"answers\"][0][\"answer_start\"] )\n",
    "\n",
    "    df = pd.DataFrame( {\"context\"      : context_list,\n",
    "                        \"question\"     : question_list,\n",
    "                        \"answers\"      : answers_list,\n",
    "                        \"answer\"      : answer_list,\n",
    "                        \"answer_start\" : answer_start_list} )\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c29918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = json_2_df('train-v1.1-es.json')\n",
    "df_dev = json_2_df('dev-v1.1-es.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "585563cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train.to_csv(\"train_SQuAD_es_11.csv\", index = False)\n",
    "#df_dev.to_csv(\"dev_SQuAD_es_11.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8600f57d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['context', 'question', 'answers', 'answer', 'answer_start'],\n",
       "        num_rows: 269\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['context', 'question', 'answers', 'answer', 'answer_start'],\n",
       "        num_rows: 269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset = load_dataset(\"csv\",\n",
    "#                       data_files={\"train\": df_train,\n",
    "#                                   \"validation\": df_dev})\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas( df_train )\n",
    "dev_dataset = Dataset.from_pandas( df_dev )\n",
    "\n",
    "dataset = datasets.DatasetDict( {\"train\":train_dataset , \"validation\":train_dataset} )\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf042f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef preprocess_function(examples):\\n    #questions = [q.strip() for q in examples[\"question\"]]\\n    inputs = tokenizer(\\n        examples[\"question\"].tolist(),\\n        examples[\"context\"].tolist(),\\n        max_length=500,\\n        truncation=\"only_second\",\\n        return_offsets_mapping=True,\\n        padding=\"max_length\",\\n    )\\n\\n    offset_mapping = inputs.pop(\"offset_mapping\")\\n    answers = examples[\"answers\"]\\n    start_positions = []\\n    end_positions = []\\n\\n    for i, offset in enumerate(offset_mapping):\\n        answer = answers[i]\\n        start_char = answer[\"answer_start\"]\\n        end_char = answer[\"answer_start\"] + len(answer[\"text\"])\\n        sequence_ids = inputs.sequence_ids(i)\\n\\n        # Find the start and end of the context\\n        idx = 0\\n        while sequence_ids[idx] != 1:\\n            idx += 1\\n        context_start = idx\\n        while sequence_ids[idx] == 1:\\n            idx += 1\\n        context_end = idx - 1\\n\\n        # If the answer is not fully inside the context, label it (0, 0)\\n        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\\n            start_positions.append(0)\\n            end_positions.append(0)\\n        else:\\n            # Otherwise it\\'s the start and end token positions\\n            idx = context_start\\n            while idx <= context_end and offset[idx][0] <= start_char:\\n                idx += 1\\n            start_positions.append(idx - 1)\\n\\n            idx = context_end\\n            while idx >= context_start and offset[idx][1] >= end_char:\\n                idx -= 1\\n            end_positions.append(idx + 1)\\n\\n    inputs[\"start_positions\"] = start_positions\\n    inputs[\"end_positions\"] = end_positions\\n    return inputs\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def preprocess_function(examples):\n",
    "    #questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        examples[\"question\"].tolist(),\n",
    "        examples[\"context\"].tolist(),\n",
    "        max_length=500,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"]\n",
    "        end_char = answer[\"answer_start\"] + len(answer[\"text\"])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a52136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=1000,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        \n",
    "        #print(\"##################################################\")\n",
    "        #print(answer[\"answer_start\"])\n",
    "        #print(\"##################################################\")\n",
    "        \n",
    "        start_char = answer[\"answer_start\"]\n",
    "        end_char = answer[\"answer_start\"] + len(answer[\"text\"])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0dcf521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_data_train = preprocess_function(df_train)\n",
    "#tokenized_data_dev = preprocess_function(df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa169afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from datasets import load_dataset\n",
    "#squad = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "405af628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e283135",
   "metadata": {},
   "outputs": [],
   "source": [
    "#squad[\"train\"][\"answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c685db78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(squad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfdca479",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import datasets\n",
    "\n",
    "#train_dataset  = datasets.Dataset.from_dict(df_train)\n",
    "#test_dataset   = datasets.Dataset.from_dict(df_dev)\n",
    "#my_dataset_dict = datasets.DatasetDict( {\"train\":train_dataset , \"test\":test_dataset} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e6a6e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(my_dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e93ef08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['context', 'question', 'answers', 'answer', 'answer_start'],\n",
       "        num_rows: 269\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['context', 'question', 'answers', 'answer', 'answer_start'],\n",
       "        num_rows: 269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb1073e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.70ba/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.62ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 269\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_squad = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "tokenized_squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aaffb66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)\n",
    "#tokenized_squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b040b25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17c6dec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='bigscience/bloomz-560m', vocab_size=250680, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c38cafd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"pool_size=300\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c407ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3dd54793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josep\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 269\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 269\n",
      "  Number of trainable parameters = 559216642\n",
      "C:\\Users\\josep\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\models\\bloom\\modeling_bloom.py:656: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='269' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/269 : < :, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 10.00 GiB total capacity; 8.96 GiB already allocated; 0 bytes free; 9.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [25], line 20\u001b[0m\n\u001b[0;32m      1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./BLOOMZ-QA\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     11\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     12\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     13\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m     18\u001b[0m )\n\u001b[1;32m---> 20\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1498\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1499\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1500\u001b[0m )\n\u001b[1;32m-> 1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\trainer.py:1749\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1747\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1749\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1752\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1754\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1755\u001b[0m ):\n\u001b[0;32m   1756\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\trainer.py:2508\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2507\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2508\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2511\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\trainer.py:2540\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2539\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2540\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   2541\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2542\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2543\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\models\\bloom\\modeling_bloom.py:1217\u001b[0m, in \u001b[0;36mBloomForQuestionAnswering.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1206\u001b[0m \u001b[38;5;124;03mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1207\u001b[0m \u001b[38;5;124;03m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;124;03m    are not taken into account for computing the loss.\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1217\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1228\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1230\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqa_outputs(sequence_output)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\models\\bloom\\modeling_bloom.py:745\u001b[0m, in \u001b[0;36mBloomModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, **deprecated_arguments)\u001b[0m\n\u001b[0;32m    737\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    738\u001b[0m         create_custom_forward(block),\n\u001b[0;32m    739\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    742\u001b[0m         head_mask[i],\n\u001b[0;32m    743\u001b[0m     )\n\u001b[0;32m    744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 745\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m        \u001b[49m\u001b[43malibi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malibi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    755\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\models\\bloom\\modeling_bloom.py:440\u001b[0m, in \u001b[0;36mBloomBlock.forward\u001b[1;34m(self, hidden_states, alibi, attention_mask, layer_past, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    437\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    439\u001b[0m \u001b[38;5;66;03m# Self attention.\u001b[39;00m\n\u001b[1;32m--> 440\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayernorm_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43malibi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malibi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    453\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\transformers\\models\\bloom\\modeling_bloom.py:334\u001b[0m, in \u001b[0;36mBloomAttention.forward\u001b[1;34m(self, hidden_states, residual, alibi, attention_mask, layer_past, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    332\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m    333\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmasked_fill(attention_scores, attention_mask, torch\u001b[38;5;241m.\u001b[39mfinfo(attention_scores\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin)\n\u001b[1;32m--> 334\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(input_dtype)\n\u001b[0;32m    336\u001b[0m \u001b[38;5;66;03m# [batch_size, num_heads, q_length, kv_length]\u001b[39;00m\n\u001b[0;32m    337\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_dropout(attention_probs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-39\\lib\\site-packages\\torch\\nn\\functional.py:1843\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1841\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim)\n\u001b[0;32m   1842\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1843\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1844\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 10.00 GiB total capacity; 8.96 GiB already allocated; 0 bytes free; 9.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./BLOOMZ-QA\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_squad[\"train\"],\n",
    "    eval_dataset=tokenized_squad[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17569986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c1b846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db5c049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bf48d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"./BLOOMZ-QA\"\n",
    "\n",
    "# a) Get predictions\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "QA_input = {\n",
    "    'question': 'Why is model conversion important?',\n",
    "    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n",
    "}\n",
    "\n",
    "nlp(QA_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0756f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
